use async_trait::async_trait;
use serde_json;
use std::collections::HashMap;
use thiserror::Error;

#[cfg(feature = "llm-apis")]
use reqwest;

use crate::types::{LLMRequest, LLMResponse, PromptContext, KnowledgeEntity, VectorSimilarity};

#[derive(Error, Debug)]
pub enum PluginError {
    #[cfg(feature = "llm-apis")]
    #[error("Network error: {0}")]
    Network(#[from] reqwest::Error),
    #[error("API error: {0}")]
    Api(String),
    #[error("Serialization error: {0}")]
    Serialization(#[from] serde_json::Error),
    #[error("Configuration error: {0}")]
    Configuration(String),
}

/// Plugin trait for LLM interactions - allows swapping between different LLM providers
#[async_trait]
pub trait LLMPlugin: Send + Sync {
    async fn generate(&self, request: &LLMRequest) -> Result<LLMResponse, PluginError>;
    fn get_model_name(&self) -> String;
}

/// Plugin trait for prompt building - allows customizing prompt construction
#[async_trait]
pub trait PromptBuilderPlugin: Send + Sync {
    async fn build_prompt(&self, context: &PromptContext) -> anyhow::Result<String>;
}

/// Mock LLM plugin for testing
pub struct MockLLMPlugin {
    model_name: String,
}

impl MockLLMPlugin {
    pub fn new() -> Self {
        Self {
            model_name: "mock-llm-v1".to_string(),
        }
    }
}

#[async_trait]
impl LLMPlugin for MockLLMPlugin {
    async fn generate(&self, request: &LLMRequest) -> Result<LLMResponse, PluginError> {
        // Simulate processing delay
        tokio::time::sleep(std::time::Duration::from_millis(100)).await;
        
        let response_content = format!(
            "Mock response to: {}\n\nBased on the provided context, this is a simulated answer that demonstrates the ReasoningAgent's capabilities. In a real implementation, this would be generated by an actual LLM like GPT-4 or Claude.",
            request.prompt.chars().take(100).collect::<String>()
        );

        Ok(LLMResponse {
            content: response_content,
            tokens_used: Some(150),
            model: self.model_name.clone(),
        })
    }

    fn get_model_name(&self) -> String {
        self.model_name.clone()
    }
}

/// OpenAI GPT plugin for real LLM interactions
#[cfg(feature = "llm-apis")]
pub struct OpenAILLMPlugin {
    api_key: String,
    model: String,
    client: reqwest::Client,
}

#[cfg(feature = "llm-apis")]
impl OpenAILLMPlugin {
    pub fn new(api_key: String, model: Option<String>) -> Self {
        Self {
            api_key,
            model: model.unwrap_or_else(|| "gpt-3.5-turbo".to_string()),
            client: reqwest::Client::new(),
        }
    }
}

#[cfg(feature = "llm-apis")]
#[async_trait]
impl LLMPlugin for OpenAILLMPlugin {
    async fn generate(&self, request: &LLMRequest) -> Result<LLMResponse, PluginError> {
        let mut payload = HashMap::new();
        payload.insert("model", self.model.clone());
        payload.insert("prompt", request.prompt.clone());
        
        if let Some(max_tokens) = request.max_tokens {
            payload.insert("max_tokens", max_tokens.to_string());
        }
        if let Some(temperature) = request.temperature {
            payload.insert("temperature", temperature.to_string());
        }

        let response = self
            .client
            .post("https://api.openai.com/v1/completions")
            .header("Authorization", format!("Bearer {}", self.api_key))
            .header("Content-Type", "application/json")
            .json(&payload)
            .send()
            .await?;

        if !response.status().is_success() {
            return Err(PluginError::Api(format!(
                "OpenAI API returned status: {}",
                response.status()
            )));
        }

        let response_data: serde_json::Value = response.json().await?;
        
        let content = response_data["choices"][0]["text"]
            .as_str()
            .ok_or_else(|| PluginError::Api("Invalid response format".to_string()))?
            .trim()
            .to_string();

        let tokens_used = response_data["usage"]["total_tokens"]
            .as_u64()
            .map(|t| t as u32);

        Ok(LLMResponse {
            content,
            tokens_used,
            model: self.model.clone(),
        })
    }

    fn get_model_name(&self) -> String {
        self.model.clone()
    }
}

/// Hugging Face plugin for open source models
#[cfg(feature = "llm-apis")]
pub struct HuggingFaceLLMPlugin {
    api_key: String,
    model: String,
    client: reqwest::Client,
}

#[cfg(feature = "llm-apis")]
impl HuggingFaceLLMPlugin {
    pub fn new(api_key: String, model: String) -> Self {
        Self {
            api_key,
            model,
            client: reqwest::Client::new(),
        }
    }
}

#[cfg(feature = "llm-apis")]
#[async_trait]
impl LLMPlugin for HuggingFaceLLMPlugin {
    async fn generate(&self, request: &LLMRequest) -> Result<LLMResponse, PluginError> {
        let mut payload = HashMap::new();
        payload.insert("inputs", request.prompt.clone());
        
        let mut parameters = HashMap::new();
        if let Some(max_tokens) = request.max_tokens {
            parameters.insert("max_new_tokens", max_tokens);
        }
        if let Some(temperature) = request.temperature {
            parameters.insert("temperature", temperature as u32);
        }
        payload.insert("parameters", serde_json::to_string(&parameters)?);

        let url = format!("https://api-inference.huggingface.co/models/{}", self.model);
        
        let response = self
            .client
            .post(&url)
            .header("Authorization", format!("Bearer {}", self.api_key))
            .header("Content-Type", "application/json")
            .json(&payload)
            .send()
            .await?;

        if !response.status().is_success() {
            return Err(PluginError::Api(format!(
                "Hugging Face API returned status: {}",
                response.status()
            )));
        }

        let response_data: serde_json::Value = response.json().await?;
        
        let content = if response_data.is_array() {
            response_data[0]["generated_text"]
                .as_str()
                .unwrap_or("No response generated")
                .to_string()
        } else {
            response_data["generated_text"]
                .as_str()
                .unwrap_or("No response generated")
                .to_string()
        };

        Ok(LLMResponse {
            content,
            tokens_used: None, // HF doesn't always provide token counts
            model: self.model.clone(),
        })
    }

    fn get_model_name(&self) -> String {
        self.model.clone()
    }
}

/// Default prompt builder that creates structured prompts from context
pub struct DefaultPromptBuilderPlugin;

impl DefaultPromptBuilderPlugin {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait]
impl PromptBuilderPlugin for DefaultPromptBuilderPlugin {
    async fn build_prompt(&self, context: &PromptContext) -> anyhow::Result<String> {
        let mut prompt = String::new();
        
        // System instruction
        prompt.push_str("You are an AI reasoning assistant. Use the provided context to answer the user's task.\n\n");
        
        // Task
        prompt.push_str(&format!("Task: {}\n\n", context.task));
        
        // Knowledge Graph context
        if !context.kg_entities.is_empty() {
            prompt.push_str("Knowledge Graph Context:\n");
            for entity in &context.kg_entities {
                prompt.push_str(&format!("- {} ({}): ", entity.entity_type, entity.id));
                for (key, value) in &entity.properties {
                    prompt.push_str(&format!("{}={}, ", key, value));
                }
                prompt.push('\n');
            }
            prompt.push('\n');
        }
        
        // Similar content
        if !context.similar_vectors.is_empty() {
            prompt.push_str("Similar Content:\n");
            for similarity in &context.similar_vectors {
                prompt.push_str(&format!(
                    "- (similarity: {:.3}) {}\n", 
                    similarity.similarity_score, 
                    similarity.content.chars().take(200).collect::<String>()
                ));
            }
            prompt.push('\n');
        }
        
        prompt.push_str("Please provide a comprehensive answer based on the above context:");
        
        Ok(prompt)
    }
}

/// Enhanced prompt builder with more sophisticated prompt engineering
pub struct EnhancedPromptBuilderPlugin;

impl EnhancedPromptBuilderPlugin {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait]
impl PromptBuilderPlugin for EnhancedPromptBuilderPlugin {
    async fn build_prompt(&self, context: &PromptContext) -> anyhow::Result<String> {
        let mut prompt = String::new();
        
        // Enhanced system instruction with reasoning guidance
        prompt.push_str(r#"You are an expert AI reasoning assistant with access to a knowledge graph and vector database.

Instructions:
1. Analyze the task carefully
2. Use the provided knowledge graph entities as factual grounding
3. Consider the similar content for additional context and patterns
4. Reason step-by-step through the problem
5. Provide a well-structured, evidence-based response
6. Cite specific entities or similar content when relevant

"#);
        
        // Task with emphasis
        prompt.push_str(&format!("## Task\n{}\n\n", context.task));
        
        // Structured knowledge context
        if !context.kg_entities.is_empty() {
            prompt.push_str("## Knowledge Graph Entities\n");
            for (i, entity) in context.kg_entities.iter().enumerate() {
                prompt.push_str(&format!("### Entity {} - {} ({})\n", i+1, entity.entity_type, entity.id));
                for (key, value) in &entity.properties {
                    prompt.push_str(&format!("- **{}**: {}\n", key, value));
                }
                prompt.push('\n');
            }
        }
        
        // Structured similar content
        if !context.similar_vectors.is_empty() {
            prompt.push_str("## Similar Content (Ranked by Relevance)\n");
            for (i, similarity) in context.similar_vectors.iter().enumerate() {
                prompt.push_str(&format!(
                    "### Content {} (Similarity: {:.3})\n{}\n\n", 
                    i+1,
                    similarity.similarity_score, 
                    similarity.content
                ));
            }
        }
        
        prompt.push_str("## Response\nProvide your analysis and answer:");
        
        Ok(prompt)
    }
}